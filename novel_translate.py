#!/usr/bin/env python3
"""
Novel Translation Script for HY-MT1.5 (GGUF) on Apple Silicon
==============================================================
ä½¿ç”¨ llama-cpp-python è¼‰å…¥ HY-MT1.5 GGUF æ¨¡å‹ï¼Œæ­é…ï¼š
  1. è¡“èªæ³¨å…¥ (Terminology Intervention) â€” å®˜æ–¹ Prompt æ ¼å¼
  2. æ»‘å‹•è¦–çª— (Sliding Window) â€” å‰æ®µè­¯æ–‡ä½œç‚ºä¸Šä¸‹æ–‡
  3. æ–‡å­¸æ€§ç¿»è­¯ â€” å°ç£ç¹é«”ä¸­æ–‡å°èªªç­†æ³•

ä¾è³´ï¼šllama-cpp-python, tqdm, huggingface_hub
ç¡¬é«”ï¼šMac Mini M4 (32GB) with Metal GPU offload

Usage:
    python novel_translate.py --input novel.txt --glossary glossary.json --output translation_output.txt
"""

import argparse
import json
import os
import re
import sys
import time
from pathlib import Path

from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL_REPO = "Mungert/Hunyuan-MT-7B-GGUF"
DEFAULT_MODEL_FILE = "Hunyuan-MT-7B-q8_0.gguf"
DEFAULT_N_CTX = 8192
DEFAULT_N_GPU_LAYERS = -1  # Full GPU offload (Metal)

# Chunking parameters
MIN_CHUNK_CHARS = 50       # Merge paragraphs shorter than this
MAX_CHUNK_CHARS = 800      # Split paragraphs longer than this
PREV_CONTEXT_CHARS = 200   # Characters of previous translation to include

# Generation parameters (official HY-MT1.5 recommendation)
GEN_PARAMS = {
    "top_k": 20,
    "top_p": 0.6,
    "repeat_penalty": 1.05,
    "temperature": 0.7,
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Model Download & Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_model(repo_id: str, filename: str) -> str:
    """Download GGUF model from HuggingFace Hub. Returns local path."""
    from huggingface_hub import hf_hub_download

    print(f"ğŸ“¦ æ­£åœ¨æª¢æŸ¥/ä¸‹è¼‰æ¨¡å‹: {repo_id}/{filename}")
    print("   ï¼ˆé¦–æ¬¡åŸ·è¡Œéœ€ä¸‹è¼‰ ~8GBï¼Œè«‹è€å¿ƒç­‰å¾…ï¼‰")
    local_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        resume_download=True,
    )
    print(f"âœ… æ¨¡å‹è·¯å¾‘: {local_path}")
    return local_path


def load_model(model_path: str, n_ctx: int = DEFAULT_N_CTX,
               n_gpu_layers: int = DEFAULT_N_GPU_LAYERS):
    """Load GGUF model with llama-cpp-python (Metal GPU offload)."""
    from llama_cpp import Llama

    print(f"ğŸ”§ æ­£åœ¨è¼‰å…¥æ¨¡å‹ (n_ctx={n_ctx}, GPU layers={n_gpu_layers})...")
    t0 = time.time()

    llm = Llama(
        model_path=model_path,
        n_ctx=n_ctx,
        n_gpu_layers=n_gpu_layers,
        verbose=False,
        # Apple Silicon specific
        use_mmap=True,
        use_mlock=False,
    )

    elapsed = time.time() - t0
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ ({elapsed:.1f}s)")
    return llm


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Input Loading
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_source_text(filepath: str) -> str:
    """Load source novel text file."""
    path = Path(filepath)
    if not path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°åŸæ–‡æª”æ¡ˆ: {filepath}")
        sys.exit(1)

    text = path.read_text(encoding="utf-8")
    print(f"ğŸ“– å·²è¼‰å…¥åŸæ–‡: {path.name} ({len(text)} å­—)")
    return text


def load_glossary(filepath: str) -> dict:
    """Load glossary JSON file (key-value pairs)."""
    path = Path(filepath)
    if not path.exists():
        print(f"âš ï¸  æ‰¾ä¸åˆ°è¡“èªè¡¨: {filepath}ï¼Œå°‡ä¸ä½¿ç”¨è¡“èªæ³¨å…¥")
        return {}

    with open(path, "r", encoding="utf-8") as f:
        glossary = json.load(f)

    print(f"ğŸ“š å·²è¼‰å…¥è¡“èªè¡¨: {len(glossary)} ç­† ({', '.join(list(glossary.keys())[:5])}...)")
    return glossary


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Smart Chunking
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def smart_chunk(text: str, min_chars: int = MIN_CHUNK_CHARS,
                max_chars: int = MAX_CHUNK_CHARS) -> list[str]:
    """
    Split source text into translation-friendly chunks.

    Strategy:
      1. Split by double newlines (paragraphs)
      2. Merge short paragraphs (< min_chars) into previous chunk
      3. Split long paragraphs (> max_chars) at sentence boundaries
    """
    # Step 1: Split into raw paragraphs
    raw_paragraphs = re.split(r'\n\s*\n', text.strip())
    raw_paragraphs = [p.strip() for p in raw_paragraphs if p.strip()]

    # If no double-newline splits, try single newlines
    if len(raw_paragraphs) <= 1 and len(text.strip()) > max_chars:
        raw_paragraphs = text.strip().split('\n')
        raw_paragraphs = [p.strip() for p in raw_paragraphs if p.strip()]

    # Step 2: Merge short paragraphs
    merged = []
    buffer = ""
    for para in raw_paragraphs:
        if buffer and len(buffer) + len(para) < min_chars:
            buffer += "\n" + para
        elif buffer and len(buffer) < min_chars:
            buffer += "\n" + para
        else:
            if buffer:
                merged.append(buffer)
            buffer = para
    if buffer:
        merged.append(buffer)

    # Step 3: Split long paragraphs at sentence boundaries
    chunks = []
    for para in merged:
        if len(para) <= max_chars:
            chunks.append(para)
        else:
            # Split at sentence-ending punctuation
            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?"\"\n])\s*', para)
            current = ""
            for sent in sentences:
                if not sent.strip():
                    continue
                if len(current) + len(sent) > max_chars and current:
                    chunks.append(current.strip())
                    current = sent
                else:
                    current += (" " if current else "") + sent
            if current.strip():
                chunks.append(current.strip())

    # Final cleanup: remove empty chunks
    chunks = [c for c in chunks if c.strip()]
    return chunks


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Prompt Construction (The Secret Sauce)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_glossary_string(glossary: dict) -> str:
    """
    Convert glossary to HY-MT1.5 terminology intervention format.

    Official format:
      åƒè€ƒä¸‹é¢çš„ç¿»è­¯ï¼š
      {source_term} ç¿»è­¯æˆ {target_term}
    """
    if not glossary:
        return ""

    lines = []
    for src, tgt in glossary.items():
        lines.append(f"{src} ç¿»è¯‘æˆ {tgt}")

    return "å‚è€ƒä¸‹é¢çš„ç¿»è¯‘ï¼š\n" + "\n".join(lines) + "\n"


def build_prompt(source_chunk: str, glossary: dict,
                 prev_translation: str = "", target_language: str = "ç¹é«”ä¸­æ–‡") -> str:
    """
    Construct the translation prompt combining:
      1. Terminology Intervention (official HY-MT1.5 format)
      2. Contextual Translation (sliding window of previous output)
      3. Literary style instructions

    The prompt follows HY-MT1.5's official contextual translation template:
      {context}
      åƒè€ƒä¸Šé¢çš„ä¿¡æ¯ï¼ŒæŠŠä¸‹é¢çš„æ–‡æœ¬ç¿»è­¯æˆ{target_language}ï¼Œ
      æ³¨æ„ä¸éœ€è¦ç¿»è­¯ä¸Šæ–‡ï¼Œä¹Ÿä¸è¦é¡å¤–è§£é‡‹ï¼š
      {source_text}
    """
    parts = []

    # Part 1: Terminology injection
    glossary_str = build_glossary_string(glossary)
    if glossary_str:
        parts.append(glossary_str)

    # Part 2: Previous translation context (sliding window)
    if prev_translation:
        # Take last N chars of previous translation
        context_text = prev_translation[-PREV_CONTEXT_CHARS:]
        # Don't cut mid-sentence: find the first sentence boundary
        first_break = 0
        for i, ch in enumerate(context_text):
            if ch in "ã€‚ï¼ï¼Ÿ\n":
                first_break = i + 1
                break
        if first_break > 0:
            context_text = context_text[first_break:]
        if context_text.strip():
            parts.append(context_text.strip())

    # Part 3: Translation instruction + source text
    context_block = "\n".join(parts)

    if context_block.strip():
        # Use contextual translation template
        # Official SC Template
        prompt = (
            f"{context_block}\n"
            f"å‚è€ƒä¸Šé¢çš„ä¿¡æ¯ï¼ŒæŠŠä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼Œ"
            f"æ³¨æ„ä¸éœ€è¦ç¿»è¯‘ä¸Šæ–‡ï¼Œä¹Ÿä¸è¦é¢å¤–è§£é‡Šï¼š\n"
            f"{source_chunk}"
        )
    else:
        # No context available (first chunk, no glossary)
        # Official SC Template
        prompt = (
            f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š\n"
            f"{source_chunk}"
        )

    return prompt


def format_for_chat(prompt: str) -> list[dict]:
    """
    Format prompt as chat messages for HY-MT1.5.
    Note: HY-MT1.5 does NOT use system prompt (per official docs).
    """
    return [{"role": "user", "content": prompt}]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Translation Engine
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_chunk(llm, source_chunk: str, glossary: dict,
                    prev_translation: str = "") -> str:
    """Translate a single chunk using the model."""
    prompt = build_prompt(source_chunk, glossary, prev_translation)
    messages = format_for_chat(prompt)

    response = llm.create_chat_completion(
        messages=messages,
        max_tokens=2048,
        **GEN_PARAMS,
    )

    # Extract the assistant's response
    result = response["choices"][0]["message"]["content"]
    return result.strip()


def translate_novel(llm, chunks: list[str], glossary: dict,
                    output_path: str) -> str:
    """
    Translate all chunks with sliding window context.
    Writes results to output file in real-time.
    """
    all_translations = []
    prev_translation = ""

    # Open output file for real-time writing
    out_path = Path(output_path)
    with open(out_path, "w", encoding="utf-8") as f_out:
        pbar = tqdm(chunks, desc="ğŸ“ ç¿»è­¯ä¸­", unit="æ®µ",
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")

        for i, chunk in enumerate(pbar):
            # Update progress bar description
            preview = chunk[:30].replace('\n', ' ')
            pbar.set_postfix_str(f"ã€Œ{preview}...ã€")

            # Translate with context injection
            translated = translate_chunk(llm, chunk, glossary, prev_translation)

            # Write to file immediately
            f_out.write(translated)
            f_out.write("\n\n")
            f_out.flush()

            # Update sliding window
            all_translations.append(translated)
            prev_translation = translated

    full_translation = "\n\n".join(all_translations)
    return full_translation


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Main Entry Point
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(
        description="HY-MT1.5 å°èªªç¿»è­¯è…³æœ¬ (GGUF + Apple Silicon)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  python novel_translate.py --input novel.txt
  python novel_translate.py --input novel.txt --glossary glossary.json
  python novel_translate.py --input novel.txt --glossary glossary.json --output my_translation.txt
  python novel_translate.py --input novel.txt --model-path /path/to/custom.gguf
        """,
    )

    parser.add_argument("--input", "-i", required=True,
                        help="åŸæ–‡å°èªªæ–‡å­—æª”è·¯å¾‘ (novel.txt)")
    parser.add_argument("--glossary", "-g", default="glossary.json",
                        help="è¡“èªè¡¨ JSON æª”è·¯å¾‘ (é è¨­: glossary.json)")
    parser.add_argument("--output", "-o", default="translation_output.txt",
                        help="è¼¸å‡ºè­¯æ–‡æª”è·¯å¾‘ (é è¨­: translation_output.txt)")
    parser.add_argument("--model-path", default=None,
                        help="GGUF æ¨¡å‹æª”è·¯å¾‘ (ä¸æŒ‡å®šå‰‡è‡ªå‹•ä¸‹è¼‰)")
    parser.add_argument("--model-repo", default=DEFAULT_MODEL_REPO,
                        help=f"HuggingFace æ¨¡å‹ repo (é è¨­: {DEFAULT_MODEL_REPO})")
    parser.add_argument("--model-file", default=DEFAULT_MODEL_FILE,
                        help=f"GGUF æª”å (é è¨­: {DEFAULT_MODEL_FILE})")
    parser.add_argument("--n-ctx", type=int, default=DEFAULT_N_CTX,
                        help=f"ä¸Šä¸‹æ–‡è¦–çª—å¤§å° (é è¨­: {DEFAULT_N_CTX})")
    parser.add_argument("--n-gpu-layers", type=int, default=DEFAULT_N_GPU_LAYERS,
                        help="GPU offload å±¤æ•¸ (-1=å…¨éƒ¨, é è¨­: -1)")
    parser.add_argument("--max-chunk-chars", type=int, default=MAX_CHUNK_CHARS,
                        help=f"æ¯æ®µæœ€å¤§å­—æ•¸ (é è¨­: {MAX_CHUNK_CHARS})")

    args = parser.parse_args()

    # Banner
    print("=" * 60)
    print("  ğŸ“– HY-MT1.5 å°èªªç¿»è­¯è…³æœ¬")
    print("  ğŸ Apple Silicon (Metal) Optimized")
    print("  ğŸ”§ Powered by llama-cpp-python + GGUF")
    print("=" * 60)
    print()

    # Step 1: Load or download model
    if args.model_path:
        model_path = args.model_path
        if not Path(model_path).exists():
            print(f"âŒ æŒ‡å®šçš„æ¨¡å‹æª”ä¸å­˜åœ¨: {model_path}")
            sys.exit(1)
    else:
        model_path = download_model(args.model_repo, args.model_file)

    # Step 2: Load model
    llm = load_model(model_path, n_ctx=args.n_ctx,
                     n_gpu_layers=args.n_gpu_layers)

    # Step 3: Load inputs
    source_text = load_source_text(args.input)
    glossary = load_glossary(args.glossary)

    # Step 4: Smart chunking
    print(f"\nâœ‚ï¸  æ­£åœ¨é€²è¡Œæ™ºæ…§åˆ†æ®µ (max_chars={args.max_chunk_chars})...")
    chunks = smart_chunk(source_text, max_chars=args.max_chunk_chars)
    print(f"   åˆ†ç‚º {len(chunks)} æ®µ")

    # Show chunk preview
    for i, c in enumerate(chunks[:3]):
        preview = c[:60].replace('\n', 'â†µ')
        print(f"   [{i+1}] {preview}...")
    if len(chunks) > 3:
        print(f"   ... å…± {len(chunks)} æ®µ")

    # Step 5: Translate
    print(f"\nğŸš€ é–‹å§‹ç¿»è­¯ â†’ {args.output}")
    print(f"   è¡“èªæ³¨å…¥: {'âœ… ' + str(len(glossary)) + ' ç­†' if glossary else 'âŒ ç„¡'}")
    print(f"   æ»‘å‹•è¦–çª—: âœ… å‰æ®µ {PREV_CONTEXT_CHARS} å­—")
    print()

    t_start = time.time()
    full_translation = translate_novel(llm, chunks, glossary, args.output)
    t_total = time.time() - t_start

    # Summary
    print()
    print("=" * 60)
    print(f"  âœ… ç¿»è­¯å®Œæˆï¼")
    print(f"  ğŸ“„ è¼¸å‡ºæª”æ¡ˆ: {args.output}")
    print(f"  ğŸ“Š å…± {len(chunks)} æ®µ / {len(full_translation)} å­—")
    print(f"  â±ï¸  ç¸½è€—æ™‚: {t_total:.1f}s ({t_total/60:.1f} åˆ†é˜)")
    print(f"  ğŸ“ˆ å¹³å‡é€Ÿåº¦: {len(full_translation)/t_total:.0f} å­—/ç§’")
    print("=" * 60)


if __name__ == "__main__":
    main()
